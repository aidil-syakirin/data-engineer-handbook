{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "07c4464e-b7b1-416b-8015-397491740a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a38c5c3-a508-40e1-9a52-b7c223a1b82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.functions.{broadcast, split, lit}\n",
       "import java.sql.Date\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6cd297b4\n",
       "res39: org.apache.spark.sql.DataFrame = [key: string, value: string]\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.{broadcast, split, lit}\n",
    "import java.sql.Date\n",
    "val spark = SparkSession.builder().appName(\"Jupyter\").getOrCreate()\n",
    "//explicitly set the iceberg connection due to connection lost after several minutes\n",
    "spark.sql(\"SET spark.sql.catalog.my_catalog.uri=http://192.168.56.1:8181\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0468ab3-1d29-4aea-a3f5-859f3168b2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "match_detailsBucketed: org.apache.spark.sql.DataFrame = [match_id: string, player_gamertag: string ... 34 more fields]\n",
       "matchesBucketed: org.apache.spark.sql.DataFrame = [match_id: string, mapid: string ... 8 more fields]\n",
       "me_ma_plBucketed: org.apache.spark.sql.DataFrame = [match_id: string, player_gamertag: string ... 2 more fields]\n",
       "medalsBucketed: org.apache.spark.sql.DataFrame = [medal_id: bigint, sprite_uri: string ... 10 more fields]\n",
       "mapsBucketed: org.apache.spark.sql.DataFrame = [mapid: string, name: string ... 1 more field]\n",
       "matchDetailsDDL: String =\n",
       "\"\n",
       "CREATE TABLE IF NOT EXISTS bootcamp.match_details_bucketed (\n",
       "     match_id STRING,\n",
       "     player_gamertag STRING,\n",
       "     player_total_kills INTEGER,\n",
       "     player_total_deaths INTEGER\n",
       " )\n",
       " USING iceberg\n",
       " PARTITIONED BY (bucket(4, match_id));...\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//read the csv files first\n",
    "\n",
    "val match_detailsBucketed = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/match_details.csv\")\n",
    "\n",
    "val matchesBucketed = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/matches.csv\")\n",
    "                        \n",
    "val me_ma_plBucketed = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/medals_matches_players.csv\")\n",
    "\n",
    "val medalsBucketed = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/medals.csv\")\n",
    "                        \n",
    "val mapsBucketed = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/maps.csv\")\n",
    "\n",
    "// creating tables and populates them with the csv data\n",
    "\n",
    "// matches details\n",
    "\n",
    "val matchDetailsDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.match_details_bucketed (\n",
    "     match_id STRING,\n",
    "     player_gamertag STRING,\n",
    "     player_total_kills INTEGER,\n",
    "     player_total_deaths INTEGER\n",
    " )\n",
    " USING iceberg\n",
    " PARTITIONED BY (bucket(4, match_id));\n",
    "\"\"\"\n",
    " spark.sql(matchDetailsDDL)\n",
    "\n",
    " match_detailsBucketed.select(\n",
    "     $\"match_id\", $\"player_gamertag\", $\"player_total_kills\", $\"player_total_deaths\")\n",
    "     .write.mode(\"append\")\n",
    "   .bucketBy(4, \"match_id\").saveAsTable(\"bootcamp.match_details_bucketed\")\n",
    "\n",
    "\n",
    "// matches table\n",
    "                        \n",
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp.matches_bucketed\"\"\")\n",
    "val matchesDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.matches_bucketed (\n",
    "    match_id STRING,\n",
    "    mapid string,\n",
    "    is_team_game BOOLEAN,\n",
    "    playlist_id STRING,\n",
    "    completion_date TIMESTAMP\n",
    " )\n",
    " USING iceberg\n",
    " PARTITIONED BY (completion_date, bucket(4, match_id));\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(matchesDDL)\n",
    "\n",
    "// Break large dataset into smaller chunks\n",
    "val processedData = matchesBucketed\n",
    "  .select(\"match_id\", \"mapid\", \"is_team_game\", \"playlist_id\", \"completion_date\")\n",
    "\n",
    "// Process in batches if extremely large\n",
    "val distinctDates = processedData.select(\"completion_date\").distinct().collect()\n",
    "\n",
    "distinctDates.foreach { dateRow =>\n",
    "  val specificDate = dateRow.getAs[java.sql.Timestamp](0)\n",
    "  \n",
    "  processedData\n",
    "    .filter(col(\"completion_date\") === specificDate)\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"completion_date\")\n",
    "    .bucketBy(4, \"match_id\")\n",
    "    .saveAsTable(\"bootcamp.matches_bucketed\")\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "//medal_maps_players table\n",
    "\n",
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp.me_ma_pl_bucketed\"\"\")\n",
    "val memapldDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.me_ma_pl_bucketed (\n",
    "    match_id STRING,\n",
    "    player_gamertag STRING,\n",
    "    medal_id bigint,\n",
    "    count integer\n",
    " )\n",
    " USING iceberg\n",
    " partitioned by (bucket(4,match_id));\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(memapldDDL)\n",
    "\n",
    "me_ma_plBucketed.select(\n",
    "     $\"match_id\", $\"player_gamertag\", $\"medal_id\", $\"count\"\n",
    "     )\n",
    "     .write.mode(\"append\")\n",
    "     .bucketBy(4,\"match_id\")\n",
    "   .saveAsTable(\"bootcamp.me_ma_pl_bucketed\")\n",
    "\n",
    "\n",
    "//medals table\n",
    "\n",
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp.medals_bucketed\"\"\")\n",
    "val medalsDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.medals_bucketed (\n",
    "    medal_id STRING,\n",
    "    name STRING\n",
    " )\n",
    " USING iceberg;\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(medalsDDL)\n",
    "\n",
    "medalsBucketed.select(\n",
    "     $\"medal_id\", $\"name\"\n",
    "     )\n",
    "     .write.mode(\"append\")\n",
    "   .saveAsTable(\"bootcamp.medals_bucketed\")\n",
    "\n",
    "\n",
    "//maps table\n",
    "\n",
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp.maps_bucketed\"\"\")\n",
    "val mapsDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.maps_bucketed (\n",
    "    mapid STRING,\n",
    "    name STRING\n",
    " )\n",
    " USING iceberg;\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(mapsDDL)\n",
    "\n",
    "mapsBucketed.select(\n",
    "     $\"mapid\", $\"name\"\n",
    "     )\n",
    "     .write.mode(\"append\")\n",
    "   .saveAsTable(\"bootcamp.maps_bucketed\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3404f7d5-78d2-41d0-9c04-454a082c0a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------------+------------------+-------------------+\n",
      "|            match_id|    completion_date|player_gamertag|player_total_kills|player_total_deaths|\n",
      "+--------------------+-------------------+---------------+------------------+-------------------+\n",
      "|00169217-cca6-4b4...|2016-03-13 00:00:00|  King Terror V|                14|                  7|\n",
      "|00169217-cca6-4b4...|2016-03-13 00:00:00|      King Sope|                11|                  5|\n",
      "|00169217-cca6-4b4...|2016-03-13 00:00:00|       mcnaeric|                10|                 14|\n",
      "|00169217-cca6-4b4...|2016-03-13 00:00:00|    EXTREMENOVA|                 8|                 10|\n",
      "|00169217-cca6-4b4...|2016-03-13 00:00:00| Psych0ticCamel|                 8|                 14|\n",
      "|00169217-cca6-4b4...|2016-03-13 00:00:00|Trap Lord David|                 8|                 12|\n",
      "|00169217-cca6-4b4...|2016-03-13 00:00:00|       DJ RAHHH|                13|                 11|\n",
      "|00169217-cca6-4b4...|2016-03-13 00:00:00|  King Terror V|                14|                  7|\n",
      "|00169217-cca6-4b4...|2016-03-13 00:00:00|      King Sope|                11|                  5|\n",
      "|00169217-cca6-4b4...|2016-03-13 00:00:00|       mcnaeric|                10|                 14|\n",
      "+--------------------+-------------------+---------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bucketedMatches: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [match_id: string, is_team_game: boolean ... 2 more fields]\n",
       "bucketedMatchDetails: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [match_id: string, player_gamertag: string ... 2 more fields]\n",
       "bucketedJoin: org.apache.spark.sql.DataFrame = [match_id: string, completion_date: timestamp ... 3 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "val bucketedMatches = spark.table(\"bootcamp.matches_bucketed\").sort(\"match_id\")\n",
    "val bucketedMatchDetails = spark.table(\"bootcamp.match_details_bucketed\").sort(\"match_id\")\n",
    "\n",
    "val bucketedJoin = bucketedMatches.as(\"m\")\n",
    "   .join(bucketedMatchDetails.as(\"md\"), $\"m.match_id\" === $\"md.match_id\")\n",
    "   .select($\"m.match_id\", $\"m.completion_date\", $\"md.player_gamertag\", $\"md.player_total_kills\", $\"md.player_total_deaths\")\n",
    "// .take(5)\n",
    "\n",
    "bucketedJoin.createOrReplaceTempView(\"bucketed_result\")\n",
    "spark.sql(\"SELECT * FROM bucketed_result LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b309614-335f-4655-9966-a5e5ba13d779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 115 in stage 1087.0 failed 1 times, most recent failure: Lost task 115.0 in stage 1087.0 (TID 4754) (e68dbfc8961f executor driver): software.amazon.awssdk.core.exception.SdkClientException: Received an UnknownHostException when attempting to interact with a service. See cause for the exact endpoint that is failing to resolve. If this is happening on an endpoint that previously worked, there may be a network connectivity issue or your DNS cache could be storing endpoints for too long.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 115 in stage 1087.0 failed 1 times, most recent failure: Lost task 115.0 in stage 1087.0 (TID 4754) (e68dbfc8961f executor driver): software.amazon.awssdk.core.exception.SdkClientException: Received an UnknownHostException when attempting to interact with a service. See cause for the exact endpoint that is failing to resolve. If this is happening on an endpoint that previously worked, there may be a network connectivity issue or your DNS cache could be storing endpoints for too long.",
      "\tat software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:111)",
      "\tat software.amazon.awssdk.awscore.interceptor.HelpfulUnknownHostExceptionInterceptor.modifyException(HelpfulUnknownHostExceptionInterceptor.java:59)",
      "\tat software.amazon.awssdk.core.interceptor.ExecutionInterceptorChain.modifyException(ExecutionInterceptorChain.java:181)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.utils.ExceptionReportingUtils.runModifyException(ExceptionReportingUtils.java:54)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.utils.ExceptionReportingUtils.reportFailureToInterceptors(ExceptionReportingUtils.java:38)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:39)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)",
      "\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:224)",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$0(BaseSyncClientHandler.java:66)",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:60)",
      "\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:52)",
      "\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:60)",
      "\tat software.amazon.awssdk.services.s3.DefaultS3Client.getObject(DefaultS3Client.java:5174)",
      "\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:192)",
      "\tat org.apache.iceberg.aws.s3.S3InputStream.positionStream(S3InputStream.java:177)",
      "\tat org.apache.iceberg.aws.s3.S3InputStream.read(S3InputStream.java:94)",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.io.DelegatingSeekableInputStream.read(DelegatingSeekableInputStream.java:61)",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)",
      "\tat org.apache.iceberg.parquet.ReadConf.newReader(ReadConf.java:238)",
      "\tat org.apache.iceberg.parquet.ReadConf.<init>(ReadConf.java:81)",
      "\tat org.apache.iceberg.parquet.VectorizedParquetReader.init(VectorizedParquetReader.java:90)",
      "\tat org.apache.iceberg.parquet.VectorizedParquetReader.iterator(VectorizedParquetReader.java:99)",
      "\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:109)",
      "\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:41)",
      "\tat org.apache.iceberg.spark.source.BaseReader.next(BaseReader.java:143)",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:120)",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:158)",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1(DataSourceRDD.scala:63)",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1$adapted(DataSourceRDD.scala:63)",
      "\tat scala.Option.exists(Option.scala:376)",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: warehouse.minio",
      "\tat software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:111)",
      "\tat software.amazon.awssdk.core.exception.SdkClientException.create(SdkClientException.java:47)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.setLastException(RetryableStageHelper.java:223)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:83)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)",
      "\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)",
      "\t... 52 more",
      "\tSuppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 1 failure: Unable to execute HTTP request: warehouse.minio",
      "\tSuppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 2 failure: Unable to execute HTTP request: warehouse.minio",
      "\tSuppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 3 failure: Unable to execute HTTP request: warehouse.minio",
      "Caused by: java.net.UnknownHostException: warehouse.minio",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1386)",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1307)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.conn.SystemDefaultDnsResolver.resolve(SystemDefaultDnsResolver.java:45)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:112)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376)",
      "\tat software.amazon.awssdk.http.apache.internal.conn.ClientConnectionManagerFactory$DelegatingHttpClientConnectionManager.connect(ClientConnectionManagerFactory.java:86)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)",
      "\tat software.amazon.awssdk.http.apache.internal.impl.ApacheSdkHttpClient.execute(ApacheSdkHttpClient.java:72)",
      "\tat software.amazon.awssdk.http.apache.ApacheHttpClient.execute(ApacheHttpClient.java:254)",
      "\tat software.amazon.awssdk.http.apache.ApacheHttpClient.access$500(ApacheHttpClient.java:104)",
      "\tat software.amazon.awssdk.http.apache.ApacheHttpClient$1.call(ApacheHttpClient.java:231)",
      "\tat software.amazon.awssdk.http.apache.ApacheHttpClient$1.call(ApacheHttpClient.java:228)",
      "\tat software.amazon.awssdk.core.internal.util.MetricUtils.measureDurationUnsafe(MetricUtils.java:99)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.executeHttpRequest(MakeHttpRequestStage.java:79)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.execute(MakeHttpRequestStage.java:57)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.execute(MakeHttpRequestStage.java:40)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:72)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:55)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:39)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)",
      "\t... 64 more",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "Caused by: software.amazon.awssdk.core.exception.SdkClientException: Received an UnknownHostException when attempting to interact with a service. See cause for the exact endpoint that is failing to resolve. If this is happening on an endpoint that previously worked, there may be a network connectivity issue or your DNS cache could be storing endpoints for too long.",
      "  at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:111)",
      "  at software.amazon.awssdk.awscore.interceptor.HelpfulUnknownHostExceptionInterceptor.modifyException(HelpfulUnknownHostExceptionInterceptor.java:59)",
      "  at software.amazon.awssdk.core.interceptor.ExecutionInterceptorChain.modifyException(ExecutionInterceptorChain.java:181)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.ExceptionReportingUtils.runModifyException(ExceptionReportingUtils.java:54)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.ExceptionReportingUtils.reportFailureToInterceptors(ExceptionReportingUtils.java:38)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:39)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)",
      "  at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:224)",
      "  at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)",
      "  at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)",
      "  at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$0(BaseSyncClientHandler.java:66)",
      "  at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)",
      "  at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:60)",
      "  at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:52)",
      "  at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:60)",
      "  at software.amazon.awssdk.services.s3.DefaultS3Client.getObject(DefaultS3Client.java:5174)",
      "  at org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:192)",
      "  at org.apache.iceberg.aws.s3.S3InputStream.positionStream(S3InputStream.java:177)",
      "  at org.apache.iceberg.aws.s3.S3InputStream.read(S3InputStream.java:94)",
      "  at org.apache.iceberg.shaded.org.apache.parquet.io.DelegatingSeekableInputStream.read(DelegatingSeekableInputStream.java:61)",
      "  at org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)",
      "  at org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)",
      "  at org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)",
      "  at org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)",
      "  at org.apache.iceberg.parquet.ReadConf.newReader(ReadConf.java:238)",
      "  at org.apache.iceberg.parquet.ReadConf.<init>(ReadConf.java:81)",
      "  at org.apache.iceberg.parquet.VectorizedParquetReader.init(VectorizedParquetReader.java:90)",
      "  at org.apache.iceberg.parquet.VectorizedParquetReader.iterator(VectorizedParquetReader.java:99)",
      "  at org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:109)",
      "  at org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:41)",
      "  at org.apache.iceberg.spark.source.BaseReader.next(BaseReader.java:143)",
      "  at org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:120)",
      "  at org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:158)",
      "  at org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1(DataSourceRDD.scala:63)",
      "  at org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1$adapted(DataSourceRDD.scala:63)",
      "  at scala.Option.exists(Option.scala:376)",
      "  at org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)",
      "  at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)",
      "  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)",
      "  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)",
      "  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)",
      "  at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)",
      "  at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:141)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)",
      "  at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)",
      "  at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "  at java.base/java.lang.Thread.run(Thread.java:829)",
      "Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: warehouse.minio",
      "  at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:111)",
      "  at software.amazon.awssdk.core.exception.SdkClientException.create(SdkClientException.java:47)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.setLastException(RetryableStageHelper.java:223)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:83)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "  at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)",
      "  at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)",
      "  ... 52 more",
      "Caused by: java.net.UnknownHostException: warehouse.minio",
      "  at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)",
      "  at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)",
      "  at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1386)",
      "  at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1307)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.conn.SystemDefaultDnsResolver.resolve(SystemDefaultDnsResolver.java:45)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:112)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376)",
      "  at software.amazon.awssdk.http.apache.internal.conn.ClientConnectionManagerFactory$DelegatingHttpClientConnectionManager.connect(ClientConnectionManagerFactory.java:86)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)",
      "  at software.amazon.awssdk.http.apache.internal.impl.ApacheSdkHttpClient.execute(ApacheSdkHttpClient.java:72)",
      "  at software.amazon.awssdk.http.apache.ApacheHttpClient.execute(ApacheHttpClient.java:254)",
      "  at software.amazon.awssdk.http.apache.ApacheHttpClient.access$500(ApacheHttpClient.java:104)",
      "  at software.amazon.awssdk.http.apache.ApacheHttpClient$1.call(ApacheHttpClient.java:231)",
      "  at software.amazon.awssdk.http.apache.ApacheHttpClient$1.call(ApacheHttpClient.java:228)",
      "  at software.amazon.awssdk.core.internal.util.MetricUtils.measureDurationUnsafe(MetricUtils.java:99)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.executeHttpRequest(MakeHttpRequestStage.java:79)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.execute(MakeHttpRequestStage.java:57)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.execute(MakeHttpRequestStage.java:40)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:72)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:55)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:39)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)",
      "  ... 64 more",
      "  Suppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 1 failure: Unable to execute HTTP request: warehouse.minio",
      "  Suppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 2 failure: Unable to execute HTTP request: warehouse.minio",
      "  Suppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 3 failure: Unable to execute HTTP request: warehouse.minio",
      ""
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "val bucketedMatches = spark.table(\"bootcamp.matches_bucketed\").sort(\"match_id\")\n",
    "val bucketedMatchDetails = spark.table(\"bootcamp.match_details_bucketed\").sort(\"match_id\")\n",
    "val bucketedMedalMatchPlayers = spark.table(\"bootcamp.me_ma_pl_bucketed\").sort(\"match_id\")\n",
    "val bucketedMedals = spark.table(\"bootcamp.medals_bucketed\")\n",
    "val bucketedMaps = spark.table(\"bootcamp.maps_bucketed\")\n",
    "\n",
    "val bucketedJoin = bucketedMatches.as(\"m\")\n",
    "   .join(bucketedMatchDetails.as(\"md\"), $\"m.match_id\" === $\"md.match_id\")\n",
    "   .join(bucketedMedalMatchPlayers.as(\"mmp\"), $\"md.match_id\" === $\"mmp.match_id\" && $\"md.player_gamertag\" === $\"mmp.player_gamertag\")\n",
    "   .join(broadcast(bucketedMedals).as(\"me\"), $\"mmp.medal_id\" === $\"me.medal_id\")\n",
    "   .join(broadcast(bucketedMaps).as(\"mp\"), $\"m.mapid\" === $\"mp.mapid\")\n",
    "   .select($\"m.match_id\", $\"m.is_team_game\", $\"m.playlist_id\", $\"completion_date\", \n",
    "       $\"md.player_gamertag\", $\"md.player_total_kills\",  $\"md.player_total_deaths\",\n",
    "        $\"mmp.medal_id\", $\"me.name\", $\"mmp.count\",\n",
    "        $\"mp.mapid\", $\"mp.name\")\n",
    "//       .explain()\n",
    "\n",
    "bucketedJoin.createOrReplaceTempView(\"bucketed_result\")\n",
    "spark.sql(\"SELECT * FROM bucketed_result LIMIT 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "240b416a-a119-4b8a-a6c0-01199ab20576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+------------------+\n",
      "|player_gamertag|            match_id|player_total_kills|\n",
      "+---------------+--------------------+------------------+\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "|   gimpinator14|acf0e47e-20ac-4b1...|               109|\n",
      "+---------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT player_gamertag, match_id, player_total_kills FROM bucketed_result order by 3 desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5695e41-49c5-4ec1-8511-a37173c71bcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 1082.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1082.0 (TID 4629) (e68dbfc8961f executor driver): software.amazon.awssdk.core.exception.SdkClientException: Received an UnknownHostException when attempting to interact with a service. See cause for the exact endpoint that is failing to resolve. If this is happening on an endpoint that previously worked, there may be a network connectivity issue or your DNS cache could be storing endpoints for too long.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1082.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1082.0 (TID 4629) (e68dbfc8961f executor driver): software.amazon.awssdk.core.exception.SdkClientException: Received an UnknownHostException when attempting to interact with a service. See cause for the exact endpoint that is failing to resolve. If this is happening on an endpoint that previously worked, there may be a network connectivity issue or your DNS cache could be storing endpoints for too long.",
      "\tat software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:111)",
      "\tat software.amazon.awssdk.awscore.interceptor.HelpfulUnknownHostExceptionInterceptor.modifyException(HelpfulUnknownHostExceptionInterceptor.java:59)",
      "\tat software.amazon.awssdk.core.interceptor.ExecutionInterceptorChain.modifyException(ExecutionInterceptorChain.java:181)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.utils.ExceptionReportingUtils.runModifyException(ExceptionReportingUtils.java:54)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.utils.ExceptionReportingUtils.reportFailureToInterceptors(ExceptionReportingUtils.java:38)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:39)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)",
      "\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:224)",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$0(BaseSyncClientHandler.java:66)",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:60)",
      "\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:52)",
      "\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:60)",
      "\tat software.amazon.awssdk.services.s3.DefaultS3Client.getObject(DefaultS3Client.java:5174)",
      "\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:192)",
      "\tat org.apache.iceberg.aws.s3.S3InputStream.positionStream(S3InputStream.java:177)",
      "\tat org.apache.iceberg.aws.s3.S3InputStream.read(S3InputStream.java:107)",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.io.DelegatingSeekableInputStream.readFully(DelegatingSeekableInputStream.java:102)",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.io.DelegatingSeekableInputStream.readFullyHeapBuffer(DelegatingSeekableInputStream.java:127)",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.io.DelegatingSeekableInputStream.readFully(DelegatingSeekableInputStream.java:91)",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader$ConsecutivePartList.readAll(ParquetFileReader.java:1850)",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader.internalReadRowGroup(ParquetFileReader.java:990)",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:940)",
      "\tat org.apache.iceberg.parquet.VectorizedParquetReader$FileIterator.advance(VectorizedParquetReader.java:163)",
      "\tat org.apache.iceberg.parquet.VectorizedParquetReader$FileIterator.next(VectorizedParquetReader.java:141)",
      "\tat org.apache.iceberg.spark.source.BaseReader.next(BaseReader.java:138)",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:120)",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:158)",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1(DataSourceRDD.scala:63)",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1$adapted(DataSourceRDD.scala:63)",
      "\tat scala.Option.exists(Option.scala:376)",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.columnartorow_nextBatch_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: warehouse.minio",
      "\tat software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:111)",
      "\tat software.amazon.awssdk.core.exception.SdkClientException.create(SdkClientException.java:47)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.setLastException(RetryableStageHelper.java:223)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:83)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)",
      "\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)",
      "\t... 49 more",
      "\tSuppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 1 failure: Unable to execute HTTP request: warehouse.minio: Temporary failure in name resolution",
      "\tSuppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 2 failure: Unable to execute HTTP request: warehouse.minio",
      "\tSuppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 3 failure: Unable to execute HTTP request: warehouse.minio",
      "Caused by: java.net.UnknownHostException: warehouse.minio",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1386)",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1307)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.conn.SystemDefaultDnsResolver.resolve(SystemDefaultDnsResolver.java:45)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:112)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376)",
      "\tat software.amazon.awssdk.http.apache.internal.conn.ClientConnectionManagerFactory$DelegatingHttpClientConnectionManager.connect(ClientConnectionManagerFactory.java:86)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)",
      "\tat org.apache.iceberg.aws.shaded.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)",
      "\tat software.amazon.awssdk.http.apache.internal.impl.ApacheSdkHttpClient.execute(ApacheSdkHttpClient.java:72)",
      "\tat software.amazon.awssdk.http.apache.ApacheHttpClient.execute(ApacheHttpClient.java:254)",
      "\tat software.amazon.awssdk.http.apache.ApacheHttpClient.access$500(ApacheHttpClient.java:104)",
      "\tat software.amazon.awssdk.http.apache.ApacheHttpClient$1.call(ApacheHttpClient.java:231)",
      "\tat software.amazon.awssdk.http.apache.ApacheHttpClient$1.call(ApacheHttpClient.java:228)",
      "\tat software.amazon.awssdk.core.internal.util.MetricUtils.measureDurationUnsafe(MetricUtils.java:99)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.executeHttpRequest(MakeHttpRequestStage.java:79)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.execute(MakeHttpRequestStage.java:57)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.execute(MakeHttpRequestStage.java:40)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:72)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:55)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:39)",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)",
      "\t... 61 more",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "Caused by: software.amazon.awssdk.core.exception.SdkClientException: Received an UnknownHostException when attempting to interact with a service. See cause for the exact endpoint that is failing to resolve. If this is happening on an endpoint that previously worked, there may be a network connectivity issue or your DNS cache could be storing endpoints for too long.",
      "  at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:111)",
      "  at software.amazon.awssdk.awscore.interceptor.HelpfulUnknownHostExceptionInterceptor.modifyException(HelpfulUnknownHostExceptionInterceptor.java:59)",
      "  at software.amazon.awssdk.core.interceptor.ExecutionInterceptorChain.modifyException(ExecutionInterceptorChain.java:181)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.ExceptionReportingUtils.runModifyException(ExceptionReportingUtils.java:54)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.ExceptionReportingUtils.reportFailureToInterceptors(ExceptionReportingUtils.java:38)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:39)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)",
      "  at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:224)",
      "  at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)",
      "  at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)",
      "  at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$0(BaseSyncClientHandler.java:66)",
      "  at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)",
      "  at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:60)",
      "  at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:52)",
      "  at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:60)",
      "  at software.amazon.awssdk.services.s3.DefaultS3Client.getObject(DefaultS3Client.java:5174)",
      "  at org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:192)",
      "  at org.apache.iceberg.aws.s3.S3InputStream.positionStream(S3InputStream.java:177)",
      "  at org.apache.iceberg.aws.s3.S3InputStream.read(S3InputStream.java:107)",
      "  at org.apache.iceberg.shaded.org.apache.parquet.io.DelegatingSeekableInputStream.readFully(DelegatingSeekableInputStream.java:102)",
      "  at org.apache.iceberg.shaded.org.apache.parquet.io.DelegatingSeekableInputStream.readFullyHeapBuffer(DelegatingSeekableInputStream.java:127)",
      "  at org.apache.iceberg.shaded.org.apache.parquet.io.DelegatingSeekableInputStream.readFully(DelegatingSeekableInputStream.java:91)",
      "  at org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader$ConsecutivePartList.readAll(ParquetFileReader.java:1850)",
      "  at org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader.internalReadRowGroup(ParquetFileReader.java:990)",
      "  at org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:940)",
      "  at org.apache.iceberg.parquet.VectorizedParquetReader$FileIterator.advance(VectorizedParquetReader.java:163)",
      "  at org.apache.iceberg.parquet.VectorizedParquetReader$FileIterator.next(VectorizedParquetReader.java:141)",
      "  at org.apache.iceberg.spark.source.BaseReader.next(BaseReader.java:138)",
      "  at org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:120)",
      "  at org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:158)",
      "  at org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1(DataSourceRDD.scala:63)",
      "  at org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1$adapted(DataSourceRDD.scala:63)",
      "  at scala.Option.exists(Option.scala:376)",
      "  at org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)",
      "  at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)",
      "  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.columnartorow_nextBatch_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)",
      "  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)",
      "  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)",
      "  at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)",
      "  at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:141)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)",
      "  at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)",
      "  at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "  at java.base/java.lang.Thread.run(Thread.java:829)",
      "Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: warehouse.minio",
      "  at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:111)",
      "  at software.amazon.awssdk.core.exception.SdkClientException.create(SdkClientException.java:47)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.setLastException(RetryableStageHelper.java:223)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:83)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "  at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)",
      "  at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)",
      "  ... 49 more",
      "Caused by: java.net.UnknownHostException: warehouse.minio",
      "  at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)",
      "  at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)",
      "  at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1386)",
      "  at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1307)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.conn.SystemDefaultDnsResolver.resolve(SystemDefaultDnsResolver.java:45)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:112)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376)",
      "  at software.amazon.awssdk.http.apache.internal.conn.ClientConnectionManagerFactory$DelegatingHttpClientConnectionManager.connect(ClientConnectionManagerFactory.java:86)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)",
      "  at org.apache.iceberg.aws.shaded.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)",
      "  at software.amazon.awssdk.http.apache.internal.impl.ApacheSdkHttpClient.execute(ApacheSdkHttpClient.java:72)",
      "  at software.amazon.awssdk.http.apache.ApacheHttpClient.execute(ApacheHttpClient.java:254)",
      "  at software.amazon.awssdk.http.apache.ApacheHttpClient.access$500(ApacheHttpClient.java:104)",
      "  at software.amazon.awssdk.http.apache.ApacheHttpClient$1.call(ApacheHttpClient.java:231)",
      "  at software.amazon.awssdk.http.apache.ApacheHttpClient$1.call(ApacheHttpClient.java:228)",
      "  at software.amazon.awssdk.core.internal.util.MetricUtils.measureDurationUnsafe(MetricUtils.java:99)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.executeHttpRequest(MakeHttpRequestStage.java:79)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.execute(MakeHttpRequestStage.java:57)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.MakeHttpRequestStage.execute(MakeHttpRequestStage.java:40)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:72)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:55)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:39)",
      "  at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)",
      "  ... 61 more",
      "  Suppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 1 failure: Unable to execute HTTP request: warehouse.minio: Temporary failure in name resolution",
      "  Suppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 2 failure: Unable to execute HTTP request: warehouse.minio",
      "  Suppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 3 failure: Unable to execute HTTP request: warehouse.minio",
      ""
     ]
    }
   ],
   "source": [
    "// Which player averages the most kills per game?\n",
    "//val avgKillPerGame = bucketedJoin.groupBy($\"player_gamertag\")\n",
    "//  .agg(avg($\"player_total_kills\").as(\"avg_kills_per_match\"))\n",
    "//  .orderBy(avg($\"player_total_kills\").desc)\n",
    "\n",
    "//Which playlist gets played the most?\n",
    "val playlistCount = bucketedJoin.groupBy($\"playlist_id\")\n",
    "  .agg(countDistinct($\"match_id\").as(\"total_games_played\"))\n",
    "  .orderBy(countDistinct($\"match_id\").desc)\n",
    "\n",
    "\n",
    "playlistCount.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cca881b4-aa68-40fe-9061-e3ad6a8501d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+--------------------+--------------------+-------------+-------------------+--------------+---------+--------------------+\n",
      "|            match_id|               mapid|is_team_game|         playlist_id|     game_variant_id|is_match_over|    completion_date|match_duration|game_mode|      map_variant_id|\n",
      "+--------------------+--------------------+------------+--------------------+--------------------+-------------+-------------------+--------------+---------+--------------------+\n",
      "|11de1a94-8d07-416...|c7edbf0f-f206-11e...|        true|f72e0ef0-7c4a-430...|1e473914-46e4-408...|         true|2016-02-22 00:00:00|          NULL|     NULL|                NULL|\n",
      "|d3643e71-3e51-43e...|cb914b9e-f206-11e...|       false|d0766624-dbd7-453...|257a305e-4dd3-41f...|         true|2016-02-14 00:00:00|          NULL|     NULL|                NULL|\n",
      "|d78d2aae-36e4-48a...|c7edbf0f-f206-11e...|        true|f72e0ef0-7c4a-430...|1e473914-46e4-408...|         true|2016-03-24 00:00:00|          NULL|     NULL|55e5ee2e-88df-465...|\n",
      "|b440069e-ec5f-4f5...|c7edbf0f-f206-11e...|        true|f72e0ef0-7c4a-430...|1e473914-46e4-408...|         true|2015-12-23 00:00:00|          NULL|     NULL|ec3eef73-13e3-4d4...|\n",
      "|1dd475fc-ee6b-4e1...|c93d708f-f206-11e...|        true|0e39ead4-383b-445...|42f97cca-2cb4-497...|         true|2016-04-07 00:00:00|          NULL|     NULL|                NULL|\n",
      "|848f02ad-72ef-479...|cbcea2c0-f206-11e...|        true|2323b76a-db98-4e0...|257a305e-4dd3-41f...|         true|2016-03-17 00:00:00|          NULL|     NULL|                NULL|\n",
      "|e207adc1-4d7a-43a...|cc74f4e1-f206-11e...|        true|2323b76a-db98-4e0...|257a305e-4dd3-41f...|         true|2016-04-05 00:00:00|          NULL|     NULL|                NULL|\n",
      "|1fb5c2ec-ca60-434...|ca737f8f-f206-11e...|        true|bc0f8ad6-31e6-4a1...|257a305e-4dd3-41f...|         true|2015-12-16 00:00:00|          NULL|     NULL|                NULL|\n",
      "|54f1cbd2-2be6-4d5...|cbcea2c0-f206-11e...|        NULL|892189e9-d712-4bd...|257a305e-4dd3-41f...|         NULL|2016-02-04 00:00:00|          NULL|     NULL|7108c409-6d1e-41d...|\n",
      "|9e079488-1355-4c6...|c74c9d0f-f206-11e...|        true|0bcf2be1-3168-4e4...|b45854a7-e6e1-4a9...|         true|2015-11-22 00:00:00|          NULL|     NULL|1c632c30-3994-444...|\n",
      "|e17af58c-ee3d-4a7...|cbcea2c0-f206-11e...|        NULL|2323b76a-db98-4e0...|257a305e-4dd3-41f...|         NULL|2016-02-04 00:00:00|          NULL|     NULL|7108c409-6d1e-41d...|\n",
      "|22027c64-d45a-48a...|ca737f8f-f206-11e...|        NULL|d0766624-dbd7-453...|257a305e-4dd3-41f...|         NULL|2016-02-04 00:00:00|          NULL|     NULL|1ebdebca-e1f8-48f...|\n",
      "|8483565b-14d1-460...|c7805740-f206-11e...|        NULL|f72e0ef0-7c4a-430...|1e473914-46e4-408...|         NULL|2016-02-04 00:00:00|          NULL|     NULL|b5f6104e-3a99-438...|\n",
      "|bc8f0d7a-cf4b-4a2...|c7edbf0f-f206-11e...|        NULL|f72e0ef0-7c4a-430...|1e473914-46e4-408...|         NULL|2016-02-04 00:00:00|          NULL|     NULL|7248ebbd-cd40-456...|\n",
      "|f60f9d91-42c9-4a0...|cdb934b0-f206-11e...|        NULL|f72e0ef0-7c4a-430...|1e473914-46e4-408...|         NULL|2016-02-04 00:00:00|          NULL|     NULL|                NULL|\n",
      "|ad4a5b9d-7127-404...|c7edbf0f-f206-11e...|        true|f72e0ef0-7c4a-430...|1e473914-46e4-408...|         true|2016-02-13 00:00:00|          NULL|     NULL|d591e5ba-a0db-4bf...|\n",
      "|f44c9997-eb6f-4d6...|ce1dc2de-f206-11e...|        true|0504ca3c-de41-48f...|b0df8938-0fb6-42e...|         true|2016-02-28 00:00:00|          NULL|     NULL|d5a6277a-96d5-499...|\n",
      "|5d8c7557-5347-451...|cdb934b0-f206-11e...|        NULL|d0766624-dbd7-453...|257a305e-4dd3-41f...|         NULL|2016-02-04 00:00:00|          NULL|     NULL|                NULL|\n",
      "|89b24a08-8588-464...|cdb934b0-f206-11e...|        NULL|d0766624-dbd7-453...|257a305e-4dd3-41f...|         NULL|2016-02-04 00:00:00|          NULL|     NULL|                NULL|\n",
      "|01805fa5-bd7c-462...|cdb934b0-f206-11e...|        NULL|f72e0ef0-7c4a-430...|1e473914-46e4-408...|         NULL|2016-02-04 00:00:00|          NULL|     NULL|                NULL|\n",
      "+--------------------+--------------------+------------+--------------------+--------------------+-------------+-------------------+--------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "match_detailsBucketed: org.apache.spark.sql.DataFrame = [match_id: string, player_gamertag: string ... 34 more fields]\n",
       "matchesBucketed: org.apache.spark.sql.DataFrame = [match_id: string, mapid: string ... 8 more fields]\n",
       "me_ma_plBucketed: org.apache.spark.sql.DataFrame = [match_id: string, player_gamertag: string ... 2 more fields]\n",
       "medalsBucketed: org.apache.spark.sql.DataFrame = [medal_id: bigint, sprite_uri: string ... 10 more fields]\n",
       "mapsBucketed: org.apache.spark.sql.DataFrame = [mapid: string, name: string ... 1 more field]\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val match_detailsBucketed = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/match_details.csv\")\n",
    "\n",
    "val matchesBucketed = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/matches.csv\")\n",
    "                        \n",
    "val me_ma_plBucketed = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/medals_matches_players.csv\")\n",
    "\n",
    "val medalsBucketed = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/medals.csv\")\n",
    "                        \n",
    "val mapsBucketed = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/maps.csv\")\n",
    "                        \n",
    "matchesBucketed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a4cd0a-562d-4c13-a3cc-5f87cd4c3928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|player_gamertag|avg_kill|\n",
      "+---------------+--------+\n",
      "|  A 29 Delivery|    11.0|\n",
      "|  A 29 Delivery|    11.0|\n",
      "|  A 29 Delivery|    11.0|\n",
      "|    A BOOTY TAP|     2.0|\n",
      "|    A BOOTY TAP|     2.0|\n",
      "|    A BOOTY TAP|     2.0|\n",
      "|    A Baby Lynx|    12.0|\n",
      "|    A Baby Lynx|    12.0|\n",
      "|    A Baby Lynx|    12.0|\n",
      "| A Blind Kid 8P|    29.0|\n",
      "+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT player_gamertag, avg(player_total_kills) over (partition by player_gamertag) as avg_kill FROM bucketed_result LIMIT 10\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
